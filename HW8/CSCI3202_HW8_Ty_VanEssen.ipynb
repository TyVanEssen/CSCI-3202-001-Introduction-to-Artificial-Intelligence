{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CSCI 3202, Fall 2022\n",
    "# Homework 8\n",
    "# Due: Friday November 18, 2022 at 6:00 PM\n",
    "\n",
    "<br> \n",
    "\n",
    "### Your name: Ty VanEssen\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Some useful packages and libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from collections import deque\n",
    "import heapq\n",
    "import unittest\n",
    "from scipy import stats\n",
    "import copy as cp\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Maximizing an Objective Function with a Genetic Algorithm \n",
    "\n",
    "Suppose we've lost the index card with our favorite cupcake recipe. We know the ingredients of the cake, but cannot remember the exact amount of each ingredient. We decide to use a genetic algorithm to generate the  ingredient amounts. With each iteration of the genetic algorithm, we bake the cupcakes and taste-test them. We achieve our goal and stop running the genetic algorithm when we get to the actual recipe: \n",
    "\n",
    "* 1 tsp salt \n",
    "* 3 tsp baking powder \n",
    "* 2 cups all-purpose flour \n",
    "* 1 cup butter \n",
    "* 1 cup granulated sugar \n",
    "* 4 large eggs\n",
    "* 1 tsp vanilla extract\n",
    "* 1 cup buttermilk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [1, 3, 2, 1, 1, 4, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example starting state for a member of our population might look like: $state = [1, 2, 100, 36, 60, 3, 5, 50]$, which is equivalent to 1 tsp salt, 2 tsp baking powder, 100 cups of flour, 36 cups of butter, 60 cups of granulated sugar, 3 large eggs, 5 tsp vanilla, and 50 cups of buttermilk.\n",
    "\n",
    "### (1a) \n",
    "\n",
    "Write an objective function `def recipe_success(state)` that takes a single argument state, and returns the objective function value (fitness) of the state. The objective function should be maximized when a state reaches the target. You could for example define the fitness score of a particular state based on how far away each entry is from the target recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_success(state):\n",
    "    # Your code here.\n",
    "    scale = 0.25\n",
    "    return 100 * sum(map(lambda s, t: stats.norm.pdf(s, t, scale) / stats.norm.pdf(t,t, scale), state, target)) / len(target)\n",
    "\n",
    "\n",
    "    # return -1 * sum(map(lambda x,y: (x-y)**2, state, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".FF\n",
      "======================================================================\n",
      "FAIL: test_fitness_bad (__main__.Tests_Problem1a)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ty\\AppData\\Local\\Temp\\ipykernel_10568\\1848617216.py\", line 8, in test_fitness_bad\n",
      "    self.assertAlmostEqual(recipe_success([0]*8), 39.7429130)\n",
      "AssertionError: 0.02096641424406529 != 39.742913 within 7 places (39.72194658575594 difference)\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_fitness_worse (__main__.Tests_Problem1a)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ty\\AppData\\Local\\Temp\\ipykernel_10568\\1848617216.py\", line 10, in test_fitness_worse\n",
      "    self.assertAlmostEqual(recipe_success(range(9)), 31.1855578)\n",
      "AssertionError: 12.50838656569788 != 31.1855578 within 7 places (18.677171234302122 difference)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.028s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=2>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your own test case to make sure that the target element achieves\n",
    "# the fitness score you would expect it to (this will vary depending on\n",
    "# what you did here.)\n",
    "class Tests_Problem1a(unittest.TestCase):\n",
    "    def test_fitness_good(self):\n",
    "        self.assertAlmostEqual(recipe_success(target), 100)\n",
    "    def test_fitness_bad(self):\n",
    "        self.assertAlmostEqual(recipe_success([0]*8), 39.7429130)\n",
    "    def test_fitness_worse(self):\n",
    "        self.assertAlmostEqual(recipe_success(range(9)), 31.1855578)\n",
    "\n",
    "tests_to_run = unittest.TestSuite()\n",
    "tests_to_run.addTest(Tests_Problem1a(\"test_fitness_good\"))\n",
    "tests_to_run.addTest(Tests_Problem1a(\"test_fitness_bad\"))\n",
    "tests_to_run.addTest(Tests_Problem1a(\"test_fitness_worse\"))\n",
    "unittest.TextTestRunner().run(tests_to_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) \n",
    "\n",
    "Write a genetic algorithm that starts with a population of 100 randomly generated \"recipes/states/members\" and uses the objective function you wrote in **(1a)** to hopefully hit the target after a certain number of generations. \n",
    "\n",
    "Key components of your code:\n",
    "- Generate the initial population randomly from integers between 0 and 100 \n",
    "- Allow for mutations in your population with an overall probability of mutation set to p = 0.2\n",
    "- Choose 2 \"parents\" in the generation of each \"child\"\n",
    "- Choose a random split point at which to combine the two \"parents\"\n",
    "- Run the algorithm for 50 iterations (\"generations\"). Do you hit your target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9590042585891537 None\n",
      "1 12.250517171551373 (37.50000000000016, [18, 78, 2, 76, 1, 4, 3, 26])\n",
      "2 17.50037739545643 (50.0, [18, 78, 2, 76, 1, 17, 1, 1])\n",
      "3 22.3339064153227 (50.0, [89, 84, 2, 41, 17, 4, 1, 1])\n",
      "4 24.79246339040798 (62.5, [1, 78, 2, 76, 1, 17, 1, 1])\n",
      "5 27.66771498737892 (62.5, [1, 78, 2, 76, 1, 17, 1, 1])\n",
      "6 30.042924651521353 (62.5, [30, 78, 2, 38, 1, 4, 1, 1])\n",
      "7 30.7931343156638 (62.5, [18, 78, 2, 1, 1, 4, 65, 1])\n",
      "8 33.542798853035904 (62.500000000000156, [1, 68, 2, 92, 1, 4, 3, 1])\n",
      "9 33.83436767643611 (75.0, [30, 78, 2, 1, 1, 4, 1, 1])\n",
      "10 37.501034343102766 (75.0, [18, 78, 2, 1, 1, 4, 1, 1])\n",
      "11 38.37624400724522 (75.0, [18, 78, 2, 1, 1, 4, 1, 1])\n",
      "12 39.41813431566383 (75.0, [18, 78, 2, 1, 1, 4, 1, 1])\n",
      "13 42.50178913401556 (75.0, [74, 83, 2, 1, 1, 4, 1, 1])\n",
      "14 42.2936095543867 (87.5, [1, 77, 2, 1, 1, 4, 1, 1])\n",
      "15 44.04337193502528 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "16 45.00197084293903 (75.00419328284877, [1, 4, 2, 1, 1, 4, 65, 1])\n",
      "17 45.8355837284623 (87.5, [1, 64, 2, 1, 1, 4, 1, 1])\n",
      "18 46.75220846230048 (87.5, [1, 68, 2, 1, 1, 4, 1, 1])\n",
      "19 47.37754392492837 (87.5, [1, 68, 2, 1, 1, 4, 1, 1])\n",
      "20 47.96065361650978 (87.5, [1, 78, 2, 1, 1, 4, 1, 1])\n",
      "21 49.54408479310959 (87.5, [1, 69, 2, 1, 1, 4, 1, 1])\n",
      "22 51.41930843486152 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "23 53.62740414883342 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "24 53.002655745804375 (87.5, [1, 78, 2, 1, 1, 4, 1, 1])\n",
      "25 53.0026138129759 (87.5, [1, 79, 2, 1, 1, 4, 1, 1])\n",
      "26 54.41964389748945 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "27 55.08677182526949 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "28 55.91999333772686 (87.5, [1, 78, 2, 1, 1, 4, 1, 1])\n",
      "29 57.50303314126076 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "30 55.6699094720699 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "31 58.12787938755631 (87.50419328284877, [1, 2, 2, 1, 1, 4, 1, 1])\n",
      "32 57.87786540994683 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "33 58.169504121394496 (87.50419328284877, [1, 2, 2, 1, 1, 4, 1, 1])\n",
      "34 57.83635249698462 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "35 59.91929445725207 (87.50000000000016, [1, 1, 2, 1, 1, 4, 1, 1])\n",
      "36 62.54409877071912 (87.50419328284879, [1, 3, 2, 1, 1, 4, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.0, [1, 3, 2, 1, 1, 4, 1, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here \n",
    "class genetic_algorithm():\n",
    "    def __init__(self, objective_function, population_dimensions: tuple[int, int], mutation_probability: float) -> None:\n",
    "        self.population_dims = population_dimensions\n",
    "        self.grade = objective_function\n",
    "        self.mutate_p = mutation_probability\n",
    "        self.pop = self.init_population()\n",
    "\n",
    "    def init_population(self) -> np.ndarray[any, int]:\n",
    "        return np.random.randint(0, 100, size = self.population_dims)\n",
    "\n",
    "    def mutate(self, member: list[int]) -> list[int]:\n",
    "        do_mutate: bool = np.random.choice([True, False], p=[self.mutate_p, 1-self.mutate_p])\n",
    "\n",
    "        if do_mutate:\n",
    "            # print(np.random.normal(scale=10, size=len(member)))\n",
    "            mutated_gene : list[int] = np.random.choice([-2,-1-1,0,0,0,0,0,1,1,2], len(member))\n",
    "            for gene_idx in range(len(member)):\n",
    "                member[gene_idx] += mutated_gene[gene_idx]\n",
    "        \n",
    "        return member\n",
    "\n",
    "    def reproduce(self, distribution : list[float]) -> list[int]:\n",
    "        parent1, parent2 = [ self.pop[idx] for idx in np.random.choice(len(self.pop), 2, False, distribution) ]\n",
    "        split_idx = np.random.randint(low=1, high=self.population_dims[1])\n",
    "        return list(parent1[:split_idx]) + list(parent2[split_idx:])\n",
    "\n",
    "    def run_generations(self, n_generations: int, goal_grade: int):\n",
    "        best_member = None\n",
    "        for generation_idx in range(n_generations):\n",
    "            all_performance = [ self.grade(member) for member in self.pop]\n",
    "            print(generation_idx, np.mean(all_performance), best_member)\n",
    "            reproduce_p = [ grade/sum(all_performance) for grade in all_performance]\n",
    "\n",
    "            new_generation = [ self.mutate(self.reproduce(reproduce_p)) for idx in range(len(self.pop))]\n",
    "            best_member = max([(self.grade(member), member) for member in new_generation])\n",
    "\n",
    "            if best_member[0] >= goal_grade:\n",
    "                return best_member\n",
    "            else:\n",
    "                self.pop = new_generation\n",
    "            \n",
    "        \n",
    "        return max([(self.grade(member), member) for member in new_generation])\n",
    "\n",
    "a = genetic_algorithm(recipe_success, (300,8), 0.2)\n",
    "a.run_generations(5000, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c)\n",
    "\n",
    "In a couple of paragraphs, report the following:\n",
    "- How many generations did it take to hit the goal?\n",
    "- If you change the initial population size to 200, does that change the number of generations it takes to achieve the goal recipe?\n",
    "- If you change the probability of mutation, does that affect the number of generations it takes to achieve the goal recipe? How so?\n",
    "\n",
    "Alternate questions to answer if Target not hit:\n",
    "- Report whether you minimized or maximized the objective function and whether that led to any major changes in how you designed the probability of reproduction. A couple sentences here is fine.\n",
    " \n",
    "- Report how many ingredients you ended up matching. e.g. target = [0.5, 3, 2.5, 1, 1.5, 4, 1, 1.25], perhaps your algorithm achieved [1.5, 3, 8, 1, 1, 100, 56, 1, 1.25], then you would have matched 4 of the ingredient values.\n",
    " \n",
    "- Report how many iterations you tried in order to get this answer. (Don't burn up your machine in the process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2:  Calibrating a model for global mean sea level changes\n",
    "\n",
    "<img src=\"http://www.anthropocenemagazine.org/wp-content/uploads/2017/05/future-sea-levels.jpg\" width=\"250\">\n",
    "\n",
    "**Part A:** Load and plot some data.\n",
    "\n",
    "Let's load a couple of data sets.  `data_sealevel.csv` is a data set of global mean sea levels, and the other, `data_temperature.csv` is a data set of global mean temperatures. The following bullets discuss the quantities of interest. \n",
    "* `sealevel` will be a list of global mean sea levels (millimeters). This data is found in a column which resides within the `data_sealevel.csv`\n",
    "* `sealevel_sigma` will be a list of the *uncertainty* in global mean sea levels (millimeters). Use the column labeled `uncertainty` within the `data_sealevel.csv` file to obtain this data, and\n",
    "* `temperature` will be a list of global mean temperatures (degrees Celsius). This data is in the `temperature` column in the `data_temperature.csv` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the suggested code to load in the data files. Feel free to modify these as you wish, but that\n",
    "# is not necessary.\n",
    "\n",
    "year = []\n",
    "sealevel = []\n",
    "sealevel_sigma = []\n",
    "temperature = []\n",
    "\n",
    "dfSealevel = pd.read_csv(\"data_sealevel.csv\")\n",
    "dfTemperature = pd.read_csv(\"data_temperature.csv\")\n",
    "\n",
    "# We aren't doing any heavy-duty stats stuff, so let's just keep what we need as regular lists\n",
    "year = dfSealevel[\"year\"].tolist()\n",
    "sealevel = dfSealevel[\"sealevel\"].tolist()\n",
    "sealevel_sigma = dfSealevel[\"uncertainty\"].tolist()\n",
    "temperature = dfTemperature[\"temperature\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A (i):**\n",
    "\n",
    "- Make three plots for Global mean surface temperature, Sea level (mm), and Sea Level Uncertainty (mm). The x-axis for each of these plots will be the years over which this data was collected. \n",
    "\n",
    "- Plot the data points as a scatter plots, and plot the three plots side-by-side-by-side (one row, three columns of figures). The point here is learn how to customize your figures a bit more, and also because computer screens are (typically) wider than they are tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plotting code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A (ii):** How does the uncertainty in global mean sea levels change as a function of time?  When is the uncertainty the highest?  Give one reason why you think this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Part B:**  The \"out-of-box\" sea-level model\n",
    "\n",
    "In your plot from **(a)**, you should see quite an apparent relationship between increasing temperatures and rising sea levels.  Seems like someone should try to model the relationship between those two, huh?\n",
    "\n",
    "In the helper function, slr, below, a simple model for temperature-driven changes in global mean sea level (GMSL) is defined. This is the model of [Rahmstorf (2007)](http://science.sciencemag.org/content/315/5810/368).\n",
    "\n",
    "The `slr` model takes two parameters, $\\alpha$ and $T_{eq}$, and requires a time series of global mean temperatures: `slr(alpha, Teq, temperature)`.\n",
    "* `alpha` is the sensitivity of sea-level changes to changes in global temperature. The units for $\\alpha$ are millimeters of sea-level changes per year, or mm y$^{-1}$.\n",
    "* `Teq` is the equilibrium global mean temperature, with units of degrees Celsius.\n",
    "* `temperature` is the time series of global mean surface temperatures, assumed to be relative to the 1961-1990 mean.\n",
    "\n",
    "For now, you do not need to worry too much about how this model works.  It is very simple, and widely used, but the point here is that you can plug in a particular set of temperatures (the model **forcing**) and parameters ($\\alpha$ and $T_{eq}$), and out pops a time series of simulated global mean sea levels.\n",
    "\n",
    "**Our goal:**  pick good values for $\\alpha$ and $T_{eq}$, so that when we run the `slr` model using the observations of temperature (which we plotted above), the model output matches well the observations of global mean sea level (which we also plotted above).\n",
    "\n",
    "The whole process of figuring out what these good parameter values are is called **model calibration**, and it's awesome.  Model Calibration is the point of this problem. Let's have a look at why we need to do this in the first place, shall we?\n",
    "\n",
    "The default parameter choices given in the Rahmstorf (2007) paper are $\\alpha=3.4$ mm y$^{-1}$ and $T_{eq} = -0.5\\ ^{\\circ}$C.\n",
    "\n",
    "**Your task for Part B:**\n",
    "\n",
    "Make a plot that contains:\n",
    "* the observed sea level data as scatter points\n",
    "* the modeled sea levels as a line, using the temperature observations from above as the `temperature` input\n",
    "* an appropriate legend and axis labels\n",
    "* $x$ axis is years\n",
    "* $y$ axis is sea level\n",
    "\n",
    "Note that after you run the `slr` model, you will need to **normalize** the output relative to the 1961-1990 reference period.  That is because you are going to compare it against data that is also normalized against this reference period. The `years` that correspond to the model output should be the same as the `years` that correspond to the `temperature` input. Normalizing data can mean several things. Follow the steps outlined below to \"normalize\" the data in the way needed for this problem:\n",
    "- Compute the mean of the output of the slr model for the years from 1961-1990 (inclusive).\n",
    "- Subtract this value from each entry in the \"sealevel\" list (list returned by the slr function)\n",
    "\n",
    "\n",
    "Make sure that you normalize the data prior to plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def slr(alpha, Teq, temperature):\n",
    "    '''sea-level emulator of Rahmstorf 2007 (DOI: 10.1126/science.1135456)\n",
    "    Takes global mean temperature as forcing, and parameters:\n",
    "    alpha = temperature sensitivity of sea level rise, and\n",
    "    Teq   = equilibrium temperature,\n",
    "    and calculates a rise/fall in sea levels, based on whether the temperature\n",
    "    is warmer/cooler than the equilibrium temperature Teq.\n",
    "    Here, we are only worrying about alpha (for now!)'''\n",
    "\n",
    "    n_time = len(temperature)\n",
    "    deltat = 1\n",
    "    sealevel = [0]*n_time\n",
    "    sealevel[0] = -134\n",
    "    for t in range(n_time-1):\n",
    "        sealevel[t+1] = sealevel[t] + deltat*alpha*(temperature[t]-Teq)\n",
    "\n",
    "    return sealevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot above ought to show decent match for the late 1900s, but diverge a bit further back in time.\n",
    "\n",
    "**The point:**  We can do better than this \"out-of-the-box\" version of the Rahmstorf sea level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:**   Figuring out our objective function\n",
    "\n",
    "As our **objective function**, we will use the joint likelihood function of the observed sea level data, given the model simulation.  The following is a detailed description of the derivation of the objective function for a hill climbing routine. **Note, you do not need to do anything for this part other than to read about the objective function and execute the cell below, then move to part D.**\n",
    "\n",
    "For a single data point in year $i$, $y_i$, with associated uncertainty $\\sigma_i$, we can assume the likelihood for our model simulation in year $i$, $\\eta_i$, follows a normal distribution centered at the data point.  The model simulation is a **deterministic** result of our parameter choices $\\alpha$ and $T_{eq}$, so we write the likelihood as:\n",
    "\n",
    "$$L(y_i \\mid \\alpha, T_{eq}) = \\dfrac{1}{\\sqrt{2 \\pi} \\sigma_i} e^{-\\dfrac{(\\eta_i(\\alpha, T_{eq}) - y_i)^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "But that only uses a single data point.  Let's use all the data!  The **joint likelihood** is the product of all of the likelihoods associated with the individual data points. But that is the product of a lot of numbers that are less than 1, so it will be **tiny**.  Instead, we should try to optimize the **joint log-likelihood**, which is simply the (natural) logarithm of the joint likelihood function.\n",
    "\n",
    "If we assume the observational data ($y_i$) are all independent, then the joint log-likelihood is:\n",
    "\n",
    "$$l(\\mathbf{y} \\mid \\alpha, T_{eq}) = -\\dfrac{N}{2} \\log{(2\\pi)} - \\sum_{i=1}^N \\log{(\\sigma_i)} - \\dfrac{1}{2}\\sum_{i=1}^N \\left( \\dfrac{\\eta_i(\\alpha, T_{eq}) - y_i}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "where, $\\mathbf{y} = [y_1, y_2, \\ldots, y_N]$ is the entire vector (list) of sea level observations, $\\eta(\\alpha, T_{eq}) = [\\eta_1(\\alpha, T_{eq}), \\eta_2(\\alpha, T_{eq}), \\ldots, \\eta_N(\\alpha, T_{eq})]$ is the entire vector (list) of `slr` model output when the parameter values $\\alpha$ and $T_{eq}$ are used, and $N$ is the number of observations we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining our objective function**\n",
    "\n",
    "Now define a `log_likelihood(parameters, obs_mu, obs_sigma)` function:\n",
    "* `parameters`: argument that is a list of two parameter values, $[\\alpha, T_{eq}]$\n",
    "  * within the likelihood function, you will need to generate the model simulation $\\eta(\\alpha, T_{eq})$ using the input `parameters`, for comparison against the observational data\n",
    "* `obs_temp`: argument that is a time series (list) of observed global mean temperatures, that will be used to run the `slr` model. Provide a default value of `temperature` for this, because we only have one temperature data set to use, and we don't want to keep \n",
    "* `obs_mu`: argument that is a time series (list) of observed values, that will be used for comparison against the `model` output. Provide a default value of `sealevel` here, because we won't be changing the observational data.\n",
    "* `obs_sigma`: argument that is a time series (list) of the corresponding uncertainties in the observational data. Simiarly, provide a default value of `sealevel_sigma` here, so we can avoid the tedious task of sending the data set into this function.\n",
    "* all three of these inputs should be lists, and should be the same length\n",
    "* this routine should return a **single** float number, that is the joint log-likelihood of the given `model` simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the objective function. You will be using this function below when you code up hill-climbing and \n",
    "# simulated annealing routines.\n",
    "\n",
    "def log_likelihood(parameters, obs_temp=temperature, obs_mu=sealevel, obs_sigma=sealevel_sigma):\n",
    "    model = slr(alpha=parameters[0], Teq=parameters[1], temperature=temperature)\n",
    "    \n",
    "    # normalize\n",
    "    reference = (year.index(1961), year.index(1990))\n",
    "    model -= np.mean(model[reference[0]:(reference[1]+1)])\n",
    "\n",
    "    return np.sum([np.log(stats.norm.pdf(x=model, loc=obs_mu, scale=obs_sigma))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:**  Defining our class structure\n",
    "\n",
    "Now we will apply a hill-climbing algorithm to tune the $\\alpha$ and $T_{eq}$ parameters.\n",
    "\n",
    "Using our in-class lecture notebook on hill-climbing as a guide, do the following:\n",
    "\n",
    "* Define a `State` class, with attributes for the parameter values (which define the state) and the objective function value of that state.\n",
    "* Define a `Problem_hillclimb` **sub-class** of the more general class `Problem`, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * methods for `moves` (return the list of all possible moves from the current state) and `best_move` (return the move that maximizes the objective function).\n",
    "  * the `moves` available should be in proper 2-dimensional space.  Do **not** simply optimize one parameter, keeping the other fixed, then optimize the other parameter, while keeping the first fixed.  (*That method *can* work, but there are some theoretical issues that would need to be tackled, and we are not getting into that here.*) You are allowed to restrict yourself to movements along a grid, as long as you entertain steps in both the $\\alpha$ and the $T_{eq}$ directions.\n",
    "* Define the `hill_climb` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `hill_climb(problem, n_iter)`:  arguments are a `Problem_hillclimb` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a hill-climbing problem object, using this initial state, the log-likelihood objective function, and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "3. ***hill-climb!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**\n",
    "* As a benchmark, make sure that your log-likelihood is *at least* -500.\n",
    "* Your calibrated (optimized) model simulation should be going straight through the data points.\n",
    "* If this isn't the case, remember to normalize your model against the 1961-1990 reference period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**  Simulated annealing\n",
    "\n",
    "Let's re-calibrate the `slr` model. This time, we will use **simulated annealing**. Again, using our in-class activity as a guide, do the following:\n",
    "\n",
    "* Continue to use your `State` class above.\n",
    "* Define a `Problem_annealing` sub-class of the `Problem` class, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * method for `random_move`, to pick a random move **by drawing from a multivariate normal distribution**.  You should use the `stepsize` attribute as the covariance (width) for this.\n",
    "* Define the `simulated_annealing` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `simulated_annealing(problem, n_iter)`:  arguments are a `Problem_annealing` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum\n",
    "\n",
    "Subject to the above constraints, you may implement these however you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a simulated annealing problem object, using this initial state, the log-likelihood objective function, an appropriate temperature updating schedule and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "  * note that this \"temperature\" is distinct from the actual physical temperature used as input to drive the `slr` model\n",
    "3. ***anneal!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**  How does your model look when you plot it against the data? If it doesn't look good, then you failed this unit test :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F:**\n",
    "\n",
    "Briefly summarize your findings. Specifically discuss the $\\alpha$ and $T_{eq}$ parameter values you found in **Part D** and **Part E**. How do these compare to the parameters of the model given by Rahmstorf? Did your hill-climbing and/or your simulated annealing programs find a better fit than the Rahmstorf model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9baa21103835e55c3546ef35c7fb69eb0b077c590c874f19ce38c45754ebbf89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
